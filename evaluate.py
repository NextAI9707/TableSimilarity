import torch
import numpy as np
from sklearn.metrics import precision_recall_curve, auc, accuracy_score
import yaml
import json
import os
import argparse
from model import EnhancedTableSimilarityModel
from dataset import get_dataloader
from collections import defaultdict
import matplotlib.pyplot as plt
from sklearn.metrics import precision_recall_curve, auc, accuracy_score, roc_curve, f1_score

# è§£å†³ä¸­æ–‡ä¹±ç 
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False
class TableSimilarityEvaluator:
    """
    å¢å¼ºæ¨¡å‹è¯„ä¼°å™¨ï¼šé€‚é…MySQLä¸é…ç½®é©±åŠ¨
    """

    def __init__(self, model_path: str, config_path: str = "config.yml"):
        # åŠ è½½é…ç½®
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model_path = model_path

        # åŠ è½½å¢å¼ºæ¨¡å‹ï¼ˆé…ç½®é©±åŠ¨ï¼‰
        self.model = EnhancedTableSimilarityModel(config_path).to(self.device)

        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

        checkpoint = torch.load(model_path, map_location=self.device)
        if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
            self.model.load_state_dict(checkpoint['model_state_dict'])
        else:
            self.model.load_state_dict(checkpoint)
        self.model.eval()

        # åŠ è½½æµ‹è¯•æ•°æ®ï¼ˆé…ç½®é©±åŠ¨ï¼‰
        self.test_loader = get_dataloader(config_path=config_path, mode="test")

        # åŠ è½½æ ‡æ³¨
        annotations_path = self.config['data'].get('annotations_path', 'data/annotations.json')
        with open(annotations_path, 'r', encoding='utf-8') as f:
            self.annotations = json.load(f)

    def compute_all_similarities(self):
        """è®¡ç®—æ‰€æœ‰æµ‹è¯•å¯¹ç›¸ä¼¼åº¦ï¼ˆä¿®å¤bugå¹¶å¢å¼ºï¼‰"""
        all_preds = []
        all_labels = []
        all_table_pairs = []
        all_weights = []
        all_table_ids = []

        self.model.eval()
        with torch.no_grad():
            for batch in self.test_loader:
                # æ•°æ®è¿ç§»
                struct_a = batch['struct_a'].to(self.device)
                struct_b = batch['struct_b'].to(self.device)
                content_a = batch['content_a'].to(self.device)
                content_b = batch['content_b'].to(self.device)
                similarity = batch['similarity'].to(self.device)

                # å¢å¼ºæ¨¡å‹å‰å‘
                fused_a, fused_b, weights_a, weights_b, _, _ = self.model(
                    struct_a, content_a, struct_b, content_b
                )

                # è®¡ç®—ç›¸ä¼¼åº¦
                pred_sim = self.model.compute_similarity(fused_a, fused_b).cpu().numpy()
                true_sim = batch['similarity'].numpy()

                # è®°å½•æƒé‡
                if weights_a is not None:
                    weights = weights_a.cpu().numpy()
                    all_weights.extend(weights.tolist())

                all_preds.extend(pred_sim)
                all_labels.extend(true_sim)
                all_table_pairs.extend(list(zip(batch['table_a'], batch['table_b'])))

                # è®°å½•table IDç”¨äºåç»­åˆ†æ
                if 'table_id_a' in batch and 'table_id_b' in batch:
                    all_table_ids.extend(list(zip(batch['table_id_a'], batch['table_id_b'])))

        all_weights_array = np.array(all_weights) if all_weights else np.empty((0, 2))

        # ä¿®å¤ï¼šè¿”å›labelsä½œä¸ºtable_simsï¼ˆå®ƒä»¬æ˜¯åŒä¸€æ•°æ®ï¼‰
        return np.array(all_preds), np.array(all_labels), all_table_pairs, \
            all_weights_array, np.array(all_labels)

    def calculate_metrics(self):
        """è®¡ç®—è¯„ä¼°æŒ‡æ ‡ï¼ˆå¢å¼ºç‰ˆï¼šå¢åŠ ROC-AUCã€F1-Scoreç­‰ï¼‰"""
        print("\n" + "=" * 60)
        print("å¼€å§‹è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
        print("=" * 60)

        preds, labels, _, weights, table_sims = self.compute_all_similarities()

        # 1. åŸºç¡€å›å½’æŒ‡æ ‡
        mae = np.mean(np.abs(preds - labels))
        mse = np.mean((preds - labels) ** 2)
        rmse = np.sqrt(mse)

        # 2. åˆ†ç±»æŒ‡æ ‡ï¼ˆäºŒåˆ†ç±»ï¼‰
        threshold = 0.5
        binary_preds = (preds >= threshold).astype(int)
        binary_labels = (labels >= threshold).astype(int)
        accuracy = accuracy_score(binary_labels, binary_preds)

        # æ–°å¢ï¼šF1-Score
        f1 = f1_score(binary_labels, binary_preds, average='weighted', zero_division=0)

        # æ–°å¢ï¼šç²¾ç¡®ç‡ã€å¬å›ç‡
        from sklearn.metrics import precision_score, recall_score
        precision = precision_score(binary_labels, binary_preds, average='weighted', zero_division=0)
        recall = recall_score(binary_labels, binary_preds, average='weighted', zero_division=0)

        # 3. æ’åºæŒ‡æ ‡
        metrics = {
            'mae': mae,
            'rmse': rmse,
            'accuracy': accuracy,
            'f1_score': f1,
            'precision': precision,
            'recall': recall,
        }

        # 4. ROC-AUC
        if len(np.unique(binary_labels)) > 1:
            # ç¡®ä¿æœ‰æ­£è´Ÿæ ·æœ¬
            fpr, tpr, _ = roc_curve(binary_labels, preds)
            roc_auc = auc(fpr, tpr)
            metrics['roc_auc'] = roc_auc
        else:
            metrics['roc_auc'] = 0.0

        # 5. å¤šå°ºåº¦Recallå’ŒPrecision
        for k in [10, 50, 100]:
            metrics[f'Recall@{k}'] = self.recall_at_k(preds, labels, k)
            metrics[f'Precision@{k}'] = self.precision_at_k(preds, labels, k)

        # 6. mAPå’ŒnDCG
        metrics['mAP@10'] = self.mean_average_precision(preds, labels, k=10)
        metrics['mAP@50'] = self.mean_average_precision(preds, labels, k=50)
        metrics['nDCG@10'] = self.ndcg_score(preds, labels, k=10)
        metrics['nDCG@50'] = self.ndcg_score(preds, labels, k=50)

        # 7. æƒé‡åŠ¨æ€æ€§åˆ†æï¼ˆå¢å¼ºï¼‰
        if len(weights) > 0:
            struct_weights = weights[:, 0]
            content_weights = weights[:, 1]

            print("\nğŸ“Š æƒé‡åŠ¨æ€æ€§åˆ†æ:")
            print(f"  ç»“æ„æƒé‡: Î¼={struct_weights.mean():.3f}, Ïƒ={struct_weights.std():.3f}")
            print(f"  å†…å®¹æƒé‡: Î¼={content_weights.mean():.3f}, Ïƒ={content_weights.std():.3f}")

            metrics['Weight_Std'] = struct_weights.std()
            metrics['Struct_Weight_Mean'] = struct_weights.mean()
            metrics['Content_Weight_Mean'] = content_weights.mean()

            # åˆ†æé«˜ç›¸ä¼¼æ ·æœ¬çš„æƒé‡åå¥½
            high_sim_mask = labels >= 0.7
            if high_sim_mask.any():
                high_struct = struct_weights[high_sim_mask].mean()
                high_content = content_weights[high_sim_mask].mean()
                print(f"  é«˜ç›¸ä¼¼åº¦æ ·æœ¬: ç»“æ„æƒé‡={high_struct:.3f}, å†…å®¹æƒé‡={high_content:.3f}")
                metrics['HighSim_Struct_Weight'] = high_struct

            # æƒé‡ä¸è¡¨çº§ç›¸ä¼¼åº¦çš„ç›¸å…³æ€§
            if len(table_sims) == len(struct_weights):
                correlation = np.corrcoef(table_sims, struct_weights)[0, 1]
                metrics['Sim-Weight_Correlation'] = correlation
                print(f"  æƒé‡-ç›¸ä¼¼åº¦ç›¸å…³æ€§: {correlation:.3f} (æœŸæœ›>0.2)")

        # æ‰“å°ç»“æœ
        print("\n" + "=" * 60)
        print("è¯„ä¼°æŒ‡æ ‡æ±‡æ€»:")
        print("\nã€å›å½’æŒ‡æ ‡ã€‘")
        print(f"  MAE: {mae:.4f}")
        print(f"  RMSE: {rmse:.4f}")

        print("\nã€åˆ†ç±»æŒ‡æ ‡ã€‘")
        print(f"  Accuracy: {accuracy:.4f}")
        print(f"  F1-Score: {f1:.4f}")
        print(f"  Precision: {precision:.4f}")
        print(f"  Recall: {recall:.4f}")
        print(f"  ROC-AUC: {metrics['roc_auc']:.4f}")

        print("\nã€æ’åºæŒ‡æ ‡ã€‘")
        for k in [10, 50, 100]:
            print(f"  Recall@{k}: {metrics[f'Recall@{k}']:.4f}")
            print(f"  Precision@{k}: {metrics[f'Precision@{k}']:.4f}")

        print("\nã€æƒé‡åŠ¨æ€æ€§ã€‘")
        print(f"  Weight Std: {metrics.get('Weight_Std', 0):.4f}")
        print(f"  Sim-Weight Correlation: {metrics.get('Sim-Weight_Correlation', 0):.4f}")

        # å…³é”®è¯Šæ–­
        if metrics.get('Weight_Std', 0) < 0.1:
            print("\nâš ï¸ è­¦å‘Šï¼šæƒé‡æ ‡å‡†å·®è¿‡ä½ï¼Œé—¨æ§ç½‘ç»œæœªåŠ¨æ€è°ƒæ•´ï¼")
            print("å»ºè®®ï¼šæ£€æŸ¥DynamicPairGatingçš„è¾“å…¥ï¼Œç¡®ä¿table_simä¿¡å·")

        if metrics.get('Recall@10', 0) < 0.05:
            print("\nâš ï¸ è­¦å‘Šï¼šRecall@10è¿‡ä½ï¼Œæ¨¡å‹è¿‡äºä¿å®ˆï¼")
            print("å»ºè®®ï¼šé™ä½HardNegativeContrastiveLossçš„æ¸©åº¦å‚æ•°")

        print("=" * 60)

        return metrics

    # ä¿ç•™åŸæœ‰è¯„ä¼°å‡½æ•°ï¼ˆå®Œå…¨ä¸å˜ï¼‰
    def precision_at_k(self, preds, labels, k=10):
        sorted_idx = np.argsort(-preds)[:k]
        relevant = labels[sorted_idx] >= 0.5
        return relevant.sum() / k

    def recall_at_k(self, preds, labels, k=10):
        sorted_idx = np.argsort(-preds)[:k]
        relevant_retrieved = labels[sorted_idx].sum()
        total_relevant = labels.sum()
        return relevant_retrieved / (total_relevant + 1e-8)

    def mean_average_precision(self, preds, labels, k=10):
        sorted_idx = np.argsort(-preds)[:k]
        sorted_labels = labels[sorted_idx]

        precisions = []
        num_relevant = 0

        for i, label in enumerate(sorted_labels):
            if label >= 0.5:
                num_relevant += 1
                precisions.append(num_relevant / (i + 1))

        return np.mean(precisions) if precisions else 0.0

    def ndcg_score(self, preds, labels, k=10):
        sorted_idx = np.argsort(-preds)[:k]
        sorted_labels = labels[sorted_idx]

        dcg = sum((2 ** label - 1) / np.log2(i + 2) for i, label in enumerate(sorted_labels))
        ideal_labels = np.sort(labels)[::-1][:k]
        idcg = sum((2 ** label - 1) / np.log2(i + 2) for i, label in enumerate(ideal_labels))

        return dcg / (idcg + 1e-8)

    def visualize_results(self):
        """å¯è§†åŒ–è¯„ä¼°ç»“æœï¼ˆå¢å¼ºç‰ˆï¼š9å®«æ ¼å›¾ï¼‰"""
        print("\n" + "=" * 60)
        print("å¼€å§‹ç”Ÿæˆè¯„ä¼°å¯è§†åŒ–...")
        print("=" * 60)

        preds, labels, _, weights, _ = self.compute_all_similarities()

        if len(weights) == 0:
            print("âš ï¸ æ²¡æœ‰æƒé‡æ•°æ®å¯ä¾›å¯è§†åŒ–")
            return

        metrics = self.calculate_metrics()

        # åˆ›å»ºæ›´å¤§çš„ç”»å¸ƒ
        fig, axes = plt.subplots(3, 3, figsize=(20, 15))  # 3x3å¸ƒå±€

        # 1. ç›¸ä¼¼åº¦åˆ†å¸ƒå¯¹æ¯”
        axes[0, 0].hist(labels, bins=30, alpha=0.7, label='çœŸå®ç›¸ä¼¼åº¦', color='blue', density=True)
        axes[0, 0].hist(preds, bins=30, alpha=0.7, label='é¢„æµ‹ç›¸ä¼¼åº¦', color='orange', density=True)
        axes[0, 0].set_xlabel('ç›¸ä¼¼åº¦åˆ†æ•°', fontsize=11)
        axes[0, 0].set_ylabel('å¯†åº¦', fontsize=11)
        axes[0, 0].set_title('ç›¸ä¼¼åº¦åˆ†å¸ƒå¯¹æ¯”\n(è“è‰²:çœŸå®,æ©™è‰²:é¢„æµ‹)', fontsize=12, fontweight='bold')
        axes[0, 0].legend(loc='upper right')
        axes[0, 0].grid(alpha=0.3)

        # 2. Precision-Recallæ›²çº¿
        precisions, recalls, _ = precision_recall_curve(labels >= 0.5, preds)
        pr_auc = auc(recalls, precisions)
        axes[0, 1].plot(recalls, precisions, label=f'PR AUC = {pr_auc:.3f}', color='green', linewidth=2)
        axes[0, 1].set_xlabel('å¬å›ç‡', fontsize=11)
        axes[0, 1].set_ylabel('ç²¾ç¡®ç‡', fontsize=11)
        axes[0, 1].set_title('Precision-Recallæ›²çº¿', fontsize=12, fontweight='bold')
        axes[0, 1].legend()
        axes[0, 1].grid(alpha=0.3)

        # 3. ROCæ›²çº¿
        if len(np.unique(labels >= 0.5)) > 1:
            fpr, tpr, _ = roc_curve(labels >= 0.5, preds)
            roc_auc = auc(fpr, tpr)
            axes[0, 2].plot(fpr, tpr, label=f'ROC AUC = {roc_auc:.3f}', color='red', linewidth=2)
            axes[0, 2].plot([0, 1], [0, 1], 'k--', alpha=0.5)
        axes[0, 2].set_xlabel('å‡æ­£ç‡', fontsize=11)
        axes[0, 2].set_ylabel('çœŸæ­£ç‡', fontsize=11)
        axes[0, 2].set_title('ROCæ›²çº¿', fontsize=12, fontweight='bold')
        axes[0, 2].legend()
        axes[0, 2].grid(alpha=0.3)

        # 4. é—¨æ§æƒé‡åˆ†å¸ƒ
        struct_weights = weights[:, 0]
        content_weights = weights[:, 1]
        weight_std = struct_weights.std()

        axes[1, 0].hist(struct_weights, bins=20, alpha=0.7,
                        label=f'ç»“æ„æƒé‡\nÎ¼={struct_weights.mean():.3f}\nÏƒ={weight_std:.3f}',
                        color='purple', edgecolor='black')
        axes[1, 0].hist(content_weights, bins=20, alpha=0.7,
                        label=f'å†…å®¹æƒé‡\nÎ¼={content_weights.mean():.3f}',
                        color='orange', edgecolor='black')
        axes[1, 0].axvline(x=struct_weights.mean(), color='red', linestyle='--', linewidth=2)
        axes[1, 0].set_xlabel('æƒé‡å€¼', fontsize=11)
        axes[1, 0].set_ylabel('é¢‘æ•°', fontsize=11)
        axes[1, 0].set_title('é—¨æ§æƒé‡åˆ†å¸ƒ\n(Ïƒ>0.1ä¸ºæœ‰æ•ˆåŠ¨æ€)', fontsize=12, fontweight='bold')
        axes[1, 0].legend()
        axes[1, 0].grid(alpha=0.3)

        # 5. æƒé‡-ç›¸ä¼¼åº¦æ•£ç‚¹å›¾
        scatter = axes[1, 1].scatter(labels, struct_weights, c=labels, cmap='viridis', alpha=0.6, s=30)
        cbar = plt.colorbar(scatter, ax=axes[1, 1])
        cbar.set_label('çœŸå®ç›¸ä¼¼åº¦', fontsize=10)
        axes[1, 1].set_xlabel('çœŸå®ç›¸ä¼¼åº¦', fontsize=11)
        axes[1, 1].set_ylabel('ç»“æ„æƒé‡', fontsize=11)
        axes[1, 1].set_title('æƒé‡-ç›¸ä¼¼åº¦ç›¸å…³æ€§åˆ†æ', fontsize=12, fontweight='bold')

        corr = np.corrcoef(labels, struct_weights)[0, 1]
        axes[1, 1].text(0.05, 0.95, f'ç›¸å…³ç³»æ•° Ï={corr:.3f}', transform=axes[1, 1].transAxes,
                        fontsize=11, verticalalignment='top', fontweight='bold',
                        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))
        axes[1, 1].grid(alpha=0.3)

        # 6. Recall@Kæ›²çº¿
        k_values = [5, 10, 20, 50, 100]
        recalls = [metrics.get(f'Recall@{k}', 0) for k in k_values]
        precisions = [metrics.get(f'Precision@{k}', 0) for k in k_values]

        axes[1, 2].plot(k_values, recalls, marker='o', color='red', linewidth=2,
                        label='Recall', markersize=8)
        axes[1, 2].plot(k_values, precisions, marker='s', color='blue', linewidth=2,
                        label='Precision', markersize=8)
        axes[1, 2].axhline(y=0.3, color='green', linestyle='--', alpha=0.5, label='ç›®æ ‡30%')
        axes[1, 2].set_xlabel('Kå€¼', fontsize=11)
        axes[1, 2].set_ylabel('åˆ†æ•°', fontsize=11)
        axes[1, 2].set_title('Recall/Precision@Kæ›²çº¿', fontsize=12, fontweight='bold')
        axes[1, 2].legend()
        axes[1, 2].grid(alpha=0.3)

        # 7. è¯„ä¼°æŒ‡æ ‡é›·è¾¾å›¾
        ax7 = axes[2, 0]
        metric_names = ['Accuracy', 'F1-Score', 'ROC-AUC', 'nDCG@10', 'mAP@10']
        metric_values = [metrics.get(name, 0) for name in
                         ['accuracy', 'f1_score', 'roc_auc', 'nDCG@10', 'mAP@10']]

        # å½’ä¸€åŒ–åˆ°0-1
        metric_values_norm = np.clip(metric_values, 0, 1)

        # é›·è¾¾å›¾ï¼ˆä¿®å¤ï¼šä½¿ç”¨np.appendè¿›è¡Œæ‹¼æ¥ï¼‰
        angles = np.linspace(0, 2 * np.pi, len(metric_names), endpoint=False).tolist()

        # æ­£ç¡®é—­åˆæ•°ç»„ï¼šnp.appendç”¨äºnumpyæ•°ç»„ï¼Œ+=ç”¨äºPythonåˆ—è¡¨
        metric_values_norm = np.append(metric_values_norm, metric_values_norm[0])  # ä¿®å¤ï¼šä» += æ”¹ä¸º np.append
        angles += angles[:1]  # åˆ—è¡¨æ‹¼æ¥æ˜¯æ­£ç¡®çš„

        ax7.plot(angles, metric_values_norm, 'o-', linewidth=2, label='æ¨¡å‹è¡¨ç°')
        ax7.fill(angles, metric_values_norm, alpha=0.25)
        ax7.set_xticks(angles[:-1])
        ax7.set_xticklabels(metric_names, fontsize=10)
        ax7.set_ylim(0, 1)
        ax7.set_title('å…³é”®æŒ‡æ ‡é›·è¾¾å›¾', fontsize=12, fontweight='bold')
        ax7.grid(True)
        ax7.legend(loc='upper right', bbox_to_anchor=(1.2, 1.0))

        # 8. é¢„æµ‹è¯¯å·®åˆ†å¸ƒ
        errors = np.abs(preds - labels)
        axes[2, 1].hist(errors, bins=30, color='darkorange', alpha=0.7, edgecolor='black')
        axes[2, 1].axvline(x=errors.mean(), color='red', linestyle='--',
                           label=f'å¹³å‡è¯¯å·®={errors.mean():.3f}')
        axes[2, 1].set_xlabel('é¢„æµ‹è¯¯å·®', fontsize=11)
        axes[2, 1].set_ylabel('é¢‘æ•°', fontsize=11)
        axes[2, 1].set_title('é¢„æµ‹è¯¯å·®åˆ†å¸ƒ', fontsize=12, fontweight='bold')
        axes[2, 1].legend()
        axes[2, 1].grid(alpha=0.3)

        # 9. ç»¼åˆæŒ‡æ ‡æ±‡æ€»æŸ±çŠ¶å›¾
        ax9 = axes[2, 2]
        display_metrics = {
            'Accuracy': metrics['accuracy'],
            'F1-Score': metrics['f1_score'],
            'Recall@10': metrics['Recall@10'],
            'ROC-AUC': metrics['roc_auc']
        }

        names = list(display_metrics.keys())
        values = list(display_metrics.values())
        colors = ['skyblue', 'lightgreen', 'salmon', 'gold']

        bars = ax9.bar(names, values, color=colors, edgecolor='black', alpha=0.8)
        ax9.set_title('æ ¸å¿ƒæŒ‡æ ‡æ±‡æ€»', fontsize=12, fontweight='bold')
        ax9.set_ylabel('åˆ†æ•°', fontsize=11)
        ax9.tick_params(axis='x', rotation=45)

        # åœ¨æŸ±å­ä¸Šæ˜¾ç¤ºæ•°å€¼
        for bar, value in zip(bars, values):
            height = bar.get_height()
            ax9.text(bar.get_x() + bar.get_width() / 2, height + 0.01,
                     f'{value:.3f}', ha='center', va='bottom', fontsize=9, fontweight='bold')
        ax9.grid(alpha=0.3)

        plt.tight_layout()
        plt.savefig("evaluation_results_enhanced.png", dpi=300, bbox_inches='tight')
        print("\nâœ… å¯è§†åŒ–ç»“æœå·²ä¿å­˜è‡³ evaluation_results_enhanced.png")

        # è¯Šæ–­æŠ¥å‘Š
        print("\n" + "=" * 60)
        print("ğŸ“‹ æ¨¡å‹è¯Šæ–­æŠ¥å‘Š:")
        print("=" * 60)
        print(
            f"1. æƒé‡åŠ¨æ€æ€§: Ïƒ={metrics.get('Weight_Std', 0):.3f} {'âœ…æ­£å¸¸' if metrics.get('Weight_Std', 0) > 0.1 else 'âš ï¸è¿‡ä½'}")
        print(
            f"2. Recall@10: {metrics.get('Recall@10', 0):.3f} {'âœ…ä¼˜ç§€' if metrics.get('Recall@10', 0) > 0.3 else 'âš ï¸è¿‡ä½'}")
        print(
            f"3. æƒé‡-ç›¸ä¼¼åº¦ç›¸å…³æ€§: {metrics.get('Sim-Weight_Correlation', 0):.3f} {'âœ…æœ‰æ•ˆ' if metrics.get('Sim-Weight_Correlation', 0) > 0.2 else 'âš ï¸å¾®å¼±'}")
        print(
            f"4. ROC-AUC: {metrics.get('roc_auc', 0):.3f} {'âœ…ä¼˜ç§€' if metrics.get('roc_auc', 0) > 0.8 else 'âš ï¸éœ€ä¼˜åŒ–'}")
        print("=" * 60)

        return metrics


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model_path", type=str, default="models/best_model.pth",
                        help="æ¨¡å‹è·¯å¾„")
    args = parser.parse_args()

    print("\n" + "=" * 60)
    print("å¼€å§‹è¯„ä¼°å¢å¼ºæ¨¡å‹...")
    print("=" * 60)

    evaluator = TableSimilarityEvaluator(model_path=args.model_path)
    evaluator.visualize_results()


if __name__ == "__main__":
    main()
