import os
import faiss
import numpy as np
import yaml
import pickle
import torch
import torch.nn.functional as F
from model import EnhancedTableSimilarityModel
from dataset import TableSimilarityDataset
import mysql.connector  # æ–°å¢MySQLæ”¯æŒ
from typing import Dict, List


class VectorStore:
    """
    å‘é‡åº“å­˜å‚¨ï¼šåŸºäºFaissçš„ç›¸ä¼¼è¡¨å¿«é€Ÿæ£€ç´¢ï¼ˆMySQLé€‚é…ç‰ˆï¼‰
    """

    def __init__(self, config_path: str = "config.yml"):
        # åŠ è½½é…ç½®ï¼ˆå…³é”®ä¿®å¤ï¼šä»configè¯»å–æ‰€æœ‰å‚æ•°ï¼‰
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = yaml.safe_load(f)

        # å‘é‡å­˜å‚¨è·¯å¾„ï¼ˆä»é…ç½®è¯»å–ï¼‰
        self.index_path = self.config['vector_store']['path']

        # MySQLé…ç½®ï¼ˆå…³é”®ä¿®å¤ï¼šå–ä»£SQLiteï¼‰
        mysql_cfg = self.config.get('mysql', {})
        self.mysql_host = mysql_cfg.get('host', 'localhost')
        self.mysql_user = mysql_cfg.get('user', 'root')
        self.mysql_password = mysql_cfg.get('password', '')
        self.mysql_port = mysql_cfg.get('port', 3306)
        self.mysql_database = mysql_cfg.get('database', 'table_similarity')

        # ç»´åº¦ä»é…ç½®è¯»å–
        self.dim = self.config['model']['embedding_dims']['fused']

        self.index = faiss.IndexFlatIP(self.dim)
        self.table_mapping: Dict[int, str] = {}
        self.reverse_mapping: Dict[str, int] = {}
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # æ¨¡å‹è·¯å¾„ä»é…ç½®è¯»å–ï¼ˆå…³é”®ä¿®å¤ï¼šéç¡¬ç¼–ç ï¼‰
        model_cfg = self.config.get('training', {})
        self.model_path = model_cfg.get('save_path', 'models/best_model.pth')

        # åˆå§‹åŒ–å¢å¼ºæ¨¡å‹
        self.model = EnhancedTableSimilarityModel(config_path).to(self.device)

        if os.path.exists(self.model_path):
            checkpoint = torch.load(self.model_path, map_location=self.device)
            if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
                self.model.load_state_dict(checkpoint['model_state_dict'])
                print(f"âœ… åŠ è½½å¢å¼ºæ¨¡å‹: epoch={checkpoint.get('epoch', 'unknown')}")
            else:
                self.model.load_state_dict(checkpoint)
            print(f"âœ… æˆåŠŸåŠ è½½ {sum(p.numel() for p in self.model.parameters()):,} ä¸ªå‚æ•°")
        else:
            print(f"âš ï¸ è­¦å‘Šï¼šæœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶ {self.model_path}ï¼Œå°†ä½¿ç”¨éšæœºåˆå§‹åŒ–")

        self.model.eval()

    def _get_db_connection(self):
        """è·å–MySQLè¿æ¥ï¼ˆæ–°å¢æ–¹æ³•ï¼‰"""
        return mysql.connector.connect(
            host=self.mysql_host,
            user=self.mysql_user,
            password=self.mysql_password,
            port=self.mysql_port,
            database=self.mysql_database,
            charset='utf8mb4'
        )

    def build_vector_store(self):
        """æ„å»ºå¢å¼ºæ¨¡å‹çš„å‘é‡åº“ï¼ˆMySQLé€‚é…ç‰ˆï¼‰"""
        # å…³é”®ä¿®å¤ï¼šä½¿ç”¨MySQLæŸ¥è¯¢è¡¨
        conn = self._get_db_connection()
        cursor = conn.cursor()
        cursor.execute("SHOW TABLES")
        tables = cursor.fetchall()
        conn.close()

        if not tables:
            raise ValueError(f"MySQLæ•°æ®åº“ {self.mysql_database} ä¸­æ²¡æœ‰è¡¨ï¼")

        # å…³é”®ä¿®å¤ï¼šDatasetåˆå§‹åŒ–æ”¹ä¸ºé…ç½®é©±åŠ¨
        dataset = TableSimilarityDataset(config_path="config.yml", mode="train")

        all_vectors = []
        table_names = []
        failed_tables = []

        print("\nğŸ”§ å¼€å§‹ç¼–ç è¡¨å‘é‡ï¼ˆå¢å¼ºæ¨¡å‹ï¼‰...")
        self.model.eval()
        with torch.no_grad():
            for idx, table_info in enumerate(tables):
                table_name = table_info[0]
                try:
                    # åŠ è½½å•è¡¨
                    table = dataset._load_table(table_name)
                    struct = dataset._encode_structure(table)  # [10, 39]
                    content = dataset._encode_content(table)  # [32]

                    # æ„å»ºbatch
                    struct_batch = struct.unsqueeze(0).to(self.device)  # [1, 10, 39]
                    content_batch = content.unsqueeze(0).to(self.device)  # [1, 32]

                    # åœ¨ build_vector_store æ–¹æ³•ä¸­ï¼Œæ›¿æ¢æ¨¡å‹è°ƒç”¨è¡Œ
                    fused_vec, _, _, _, _, _ = self.model(
                        struct_a=struct_batch,
                        content_a=content_batch,
                        struct_b=struct_batch,  # è™šæ‹Ÿç¬¬äºŒè¡¨
                        content_b=content_batch
                    )

                    # å½’ä¸€åŒ–å¹¶å­˜å‚¨ï¼ˆæ¨¡å‹å·²å†…éƒ¨å½’ä¸€åŒ–ï¼Œæ­¤å¤„å†æ¬¡ç¡®ä¿ï¼‰
                    vector = F.normalize(fused_vec[0], p=2, dim=0).cpu().numpy()
                    all_vectors.append(vector)
                    table_names.append(table_name)

                    if idx % 50 == 0:
                        print(f"  å·²ç¼–ç  {idx + 1}/{len(tables)} ä¸ªè¡¨")

                except Exception as e:
                    print(f"âŒ è¡¨ {table_name} ç¼–ç å¤±è´¥: {e}")
                    failed_tables.append(table_name)
                    continue

        if not all_vectors:
            raise RuntimeError(f"æ²¡æœ‰ä»»ä½•è¡¨æˆåŠŸç¼–ç ï¼å¤±è´¥è¡¨: {failed_tables}")

        # æ„å»ºFaissç´¢å¼•
        vectors = np.array(all_vectors).astype(np.float32)
        self.index.add(vectors)

        # æ„å»ºæ˜ å°„å…³ç³»
        for idx, name in enumerate(table_names):
            self.table_mapping[idx] = name
            self.reverse_mapping[name] = idx

        # ä¿å­˜æ‰€æœ‰æ–‡ä»¶
        mapping_dir = "data"
        os.makedirs(mapping_dir, exist_ok=True)

        # ä¿å­˜Faissç´¢å¼•
        faiss.write_index(self.index, self.index_path)

        # ä¿å­˜æ˜ å°„
        with open(f"{mapping_dir}/table_mapping.pkl", 'wb') as f:
            pickle.dump(self.table_mapping, f)
        with open(f"{mapping_dir}/reverse_mapping.pkl", 'wb') as f:
            pickle.dump(self.reverse_mapping, f)

        print(f"âœ… å‘é‡åº“æ„å»ºå®Œæˆï¼å…±{len(table_names)}ä¸ªè¡¨ï¼Œç»´åº¦{self.dim}")
        print(f"âœ… æ–‡ä»¶å·²ä¿å­˜: {self.index_path}")

    def load_vector_store(self):
        """åŠ è½½å·²å­˜åœ¨çš„å‘é‡åº“"""
        if not os.path.exists(self.index_path):
            print(f"âš ï¸ å‘é‡åº“æ–‡ä»¶ä¸å­˜åœ¨: {self.index_path}")
            return False

        mapping_files = {
            'table_mapping': f"data/table_mapping.pkl",
            'reverse_mapping': f"data/reverse_mapping.pkl"
        }

        for name, path in mapping_files.items():
            if not os.path.exists(path):
                print(f"âš ï¸ æ˜ å°„æ–‡ä»¶ä¸å­˜åœ¨: {path}")
                return False

        try:
            self.index = faiss.read_index(self.index_path)

            with open(mapping_files['table_mapping'], 'rb') as f:
                self.table_mapping = pickle.load(f)
            with open(mapping_files['reverse_mapping'], 'rb') as f:
                self.reverse_mapping = pickle.load(f)

            print(f"âœ… å‘é‡åº“åŠ è½½æˆåŠŸï¼å…±{len(self.table_mapping)}ä¸ªè¡¨")
            return True
        except Exception as e:
            print(f"âœ— åŠ è½½å¤±è´¥: {e}")
            return False

    def search_similar_tables(self, table_name: str, top_k: int = 10):
        """æœç´¢ç›¸ä¼¼è¡¨ï¼ˆå¢å¼ºç‰ˆï¼šè¾¹ç•Œæ£€æŸ¥ï¼‰"""
        if not self.reverse_mapping:
            print("âš ï¸ åå‘æ˜ å°„æœªåŠ è½½ï¼Œè¯·å…ˆæ„å»ºå‘é‡åº“")
            return []

        if table_name not in self.reverse_mapping:
            print(f"è¡¨ '{table_name}' ä¸åœ¨å‘é‡åº“ä¸­")
            print(f"å¯ç”¨è¡¨ç¤ºä¾‹: {list(self.reverse_mapping.keys())[:10]}...")  # æ˜¾ç¤ºå‰10ä¸ª
            return []

        query_id = self.reverse_mapping[table_name]

        # è¾¹ç•Œæ£€æŸ¥
        k = min(top_k + 1, self.index.ntotal)
        if k <= 1:
            print("âš ï¸ å‘é‡åº“ä¸­åªæœ‰1ä¸ªè¡¨ï¼Œæ— æ³•æ¨è")
            return []

        query_vector = self.index.reconstruct(query_id)

        distances, indices = self.index.search(
            np.array([query_vector]).astype(np.float32),
            k
        )

        results = []
        for i in range(len(indices[0])):
            idx = int(indices[0][i])
            if idx != query_id:  # æ’é™¤è‡ªå·±
                results.append({
                    'table_name': self.table_mapping[idx],
                    'similarity': float(distances[0][i]),
                    'rank': len(results) + 1
                })

        return results[:top_k]

    def batch_search_all_pairs(self, top_k: int = 10):
        """æ‰¹é‡è®¡ç®—æ‰€æœ‰è¡¨å¯¹çš„ç›¸ä¼¼åº¦ï¼ˆä¿®å¤ç‰ˆï¼šè¾¹ç•Œæ£€æŸ¥ï¼‰"""
        if self.index.ntotal == 0:
            print("âš ï¸ å‘é‡åº“ä¸ºç©º")
            return []

        k = min(top_k + 1, self.index.ntotal)
        if k <= 1:
            print("âš ï¸ å‘é‡åº“ä¸­è¡¨æ•°é‡ä¸è¶³")
            return []

        all_vectors = self.index.reconstruct_n(0, self.index.ntotal)
        distances, indices = self.index.search(all_vectors, k)

        similar_pairs = []
        for i in range(len(indices)):
            table_a = self.table_mapping[i]
            for j in range(1, len(indices[i])):  # ä»1å¼€å§‹è·³è¿‡è‡ªå·±
                idx_b = int(indices[i][j])
                if idx_b < len(self.table_mapping):  # ä¿®å¤ï¼šè¾¹ç•Œæ£€æŸ¥
                    table_b = self.table_mapping[idx_b]
                    similarity = float(distances[i][j])
                    similar_pairs.append({
                        'table_a': table_a,
                        'table_b': table_b,
                        'similarity': similarity
                    })

        similar_pairs.sort(key=lambda x: x['similarity'], reverse=True)
        return similar_pairs


def main():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--rebuild", action="store_true", help="å¼ºåˆ¶é‡å»ºå‘é‡åº“")
    parser.add_argument("--test", type=str, help="æµ‹è¯•æŸ¥è¯¢è¡¨å")
    args = parser.parse_args()

    os.makedirs("data", exist_ok=True)
    store = VectorStore()

    # å°è¯•åŠ è½½ï¼Œå¤±è´¥åˆ™æ„å»º
    if args.rebuild or not store.load_vector_store():
        print("\n" + "=" * 60)
        print("å¼€å§‹æ„å»ºå‘é‡åº“...")
        print("=" * 60)
        store.build_vector_store()

    # æµ‹è¯•æŸ¥è¯¢
    if args.test and store.reverse_mapping:
        print(f"\næŸ¥è¯¢ä¸ '{args.test}' æœ€ç›¸ä¼¼çš„è¡¨ï¼ˆTop-5ï¼‰ï¼š")
        results = store.search_similar_tables(args.test, top_k=5)
        for r in results:
            print(f"  {r['table_name']}: {r['similarity']:.4f}")
    elif store.reverse_mapping:
        # é»˜è®¤æŸ¥è¯¢ç¬¬ä¸€ä¸ªè¡¨
        all_tables = list(store.reverse_mapping.keys())
        if all_tables:
            print(f"\næŸ¥è¯¢ä¸ '{all_tables[0]}' æœ€ç›¸ä¼¼çš„è¡¨ï¼ˆTop-5ï¼‰ï¼š")
            results = store.search_similar_tables(all_tables[0], top_k=5)
            for r in results:
                print(f"  {r['table_name']}: {r['similarity']:.4f}")

    # æ‰¹é‡åˆ†æï¼ˆå¯é€‰ï¼‰
    if store.reverse_mapping and len(store.reverse_mapping) > 1:
        print("\nå…¨å±€ç›¸ä¼¼åº¦åˆ†æï¼ˆå‰5å¯¹ï¼‰ï¼š")
        all_pairs = store.batch_search_all_pairs(top_k=5)
        for pair in all_pairs[:5]:
            print(f"  {pair['table_a']} <-> {pair['table_b']}: {pair['similarity']:.4f}")

if __name__ == "__main__":
    main()
