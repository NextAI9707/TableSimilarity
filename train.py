import numpy as np
import torch
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
import yaml
import os
from model import EnhancedTableSimilarityModel, Config
from dataset import get_dataloader
import argparse
from tqdm import tqdm
import sys
import matplotlib.pyplot as plt
from sklearn.metrics import roc_auc_score, f1_score, accuracy_score

# è§£å†³ä¸­æ–‡ä¹±ç 
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

class EarlyStoppingWithoutStop:
    """æ—©åœä¼˜åŒ–ï¼šå®Œå…¨é…ç½®åŒ–"""

    def __init__(self, config: Config):
        self.patience = config.get('training.early_stopping.patience', 10)
        self.delta = config.get('training.early_stopping.delta', 0.001)
        self.save_path = config.get('training.early_stopping.save_path', 'models/best_model.pth')
        self.best_loss = float('inf')
        self.best_epoch = 0
        self.no_improve_epochs = 0
        self.save_counter = 0
        os.makedirs(os.path.dirname(self.save_path), exist_ok=True)

    def step(self, val_loss: float, model, epoch: int) -> bool:
        if val_loss < self.best_loss - self.delta:
            self.best_loss = val_loss
            self.best_epoch = epoch
            self.no_improve_epochs = 0
            self.save_counter += 1
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'val_loss': val_loss,
                'config': model.config._config
            }, self.save_path)
            print(f"\nğŸ¯ æœ€ä½³æ¨¡å‹ä¿å­˜: Epoch {epoch} | éªŒè¯æŸå¤±: {val_loss:.4f} | ç¬¬ {self.save_counter} æ¬¡ä¿å­˜")
        else:
            self.no_improve_epochs += 1
        return True

    def get_best_model_info(self) -> dict:
        return {
            "best_loss": self.best_loss,
            "best_epoch": self.best_epoch,
            "save_counter": self.save_counter
        }


class TemperatureScheduler:
    """åŠ¨æ€æ¸©åº¦è°ƒåº¦ï¼šå®Œå…¨é…ç½®åŒ–"""

    def __init__(self, config: Config):
        self.initial_temp = config.get('training.temperature_scheduler.initial_temp', 0.07)
        self.final_temp = config.get('training.temperature_scheduler.final_temp', 0.04)
        self.decay_epochs = config.get('training.temperature_scheduler.decay_epochs', 7)

    def get_temperature(self, epoch: int) -> float:
        ratio = min(epoch / self.decay_epochs, 1.0) if self.decay_epochs > 0 else 1.0
        return self.initial_temp * (1 - ratio) + self.final_temp * ratio


class CurriculumScheduler:
    """è¯¾ç¨‹å­¦ä¹ è°ƒåº¦å™¨ï¼šå®Œå…¨é…ç½®åŒ–"""

    def __init__(self, config: Config):
        self.enabled = config.get('training.curriculum_learning.enabled', True)
        self.start_threshold = config.get('training.curriculum_learning.start_threshold', 0.7)
        self.end_threshold = config.get('training.curriculum_learning.end_threshold', 0.0)
        self.transition_epochs = config.get('training.curriculum_learning.transition_epochs', 5)

    def get_threshold(self, epoch: int) -> float:
        if not self.enabled:
            return 0.0
        if epoch < self.transition_epochs:
            return self.start_threshold
        return self.end_threshold


def train_epoch(model, dataloader, optimizer, device, epoch, temp_scheduler=None, config=None):
    """
    è®­ç»ƒä¸€ä¸ªepochï¼šå¢å¼ºç‰ˆ - å¢åŠ æŒ‡æ ‡è®¡ç®—ä¸è¿”å›
    """
    model.train()
    total_loss = 0
    num_batches = 0
    total_correct = 0
    total_samples = 0
    grad_norms = []
    weight_stats = []
    all_preds = []
    all_labels = []

    # è·å–æ—¥å¿—é—´éš”
    log_interval = config.get('training.logging.log_interval', 20) if config else 20

    pbar = tqdm(dataloader, desc=f"Epoch {epoch + 1} è®­ç»ƒ", file=sys.stdout, ncols=150)

    for batch_idx, batch in enumerate(pbar):
        # æ•°æ®è¿ç§»
        struct_a = batch['struct_a'].to(device)
        struct_b = batch['struct_b'].to(device)
        content_a = batch['content_a'].to(device)
        content_b = batch['content_b'].to(device)
        similarity = batch['similarity'].to(device)

        # é¦–æ¬¡batchè¯Šæ–­
        if epoch == 0 and batch_idx == 0:
            print(f"\n{'=' * 60}")
            print(f"ã€é¦–æ¬¡Batchè¯Šæ–­ã€‘")
            print(f"struct_a èŒƒå›´: [{struct_a.min():.3f}, {struct_a.max():.3f}]")
            print(f"content_a èŒƒå›´: [{content_a.min():.3f}, {content_a.max():.3f}]")
            print(f"ç›¸ä¼¼åº¦æ ‡ç­¾: {similarity[:5].tolist()}")
            print(f"{'=' * 60}\n")

        # æ ‡ç­¾äºŒå€¼åŒ–
        binary_labels = (similarity > 0.5).float()

        # å‰å‘ä¼ æ’­
        optimizer.zero_grad()

        loss, similarities, weights_a, weights_b = model.compute_loss(
            struct_a, content_a, struct_b, content_b, binary_labels
        )

        # Losså¼‚å¸¸æ£€æµ‹
        if torch.isnan(loss) or torch.isinf(loss):
            print(f"âš ï¸ è­¦å‘Šï¼šæ‰¹æ¬¡ {batch_idx} æŸå¤±å¼‚å¸¸: {loss.item()}, è·³è¿‡")
            num_batches += 1
            continue

        # åå‘ä¼ æ’­ä¸æ¢¯åº¦ç›‘æ§
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        if config and config.get('training.gradient_clip.enabled', True):
            max_norm = config.get('training.gradient_clip.max_norm', 10.0)
            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_norm)
            grad_norms.append(grad_norm.item())

        optimizer.step()

        # ç»Ÿè®¡ä¸æ—¥å¿—
        total_loss += loss.item()
        num_batches += 1

        # å‡†ç¡®ç‡è®¡ç®—
        preds = (similarities > 0.5).float()
        total_correct += (preds == binary_labels).sum().item()
        total_samples += binary_labels.size(0)

        # æ”¶é›†é¢„æµ‹å’Œæ ‡ç­¾ç”¨äºæŒ‡æ ‡è®¡ç®—
        all_preds.extend(similarities.detach().cpu().numpy())
        all_labels.extend(similarity.cpu().numpy())

        # æƒé‡ç»Ÿè®¡
        if weights_a is not None:
            weight_stats.append(weights_a.detach().cpu())

        # è¿›åº¦æ¡æ›´æ–°
        if batch_idx % log_interval == 0:
            if weight_stats:
                all_weights = torch.cat(weight_stats, dim=0)
                struct_weight_mean = all_weights[:, 0].mean().item()
                content_weight_mean = all_weights[:, 1].mean().item()
            else:
                struct_weight_mean = content_weight_mean = 0.0

            pbar.set_postfix({
                'Loss': f'{loss.item():.4f}',
                'Acc': f'{(preds == binary_labels).float().mean():.2%}',
                'StructW': f'{struct_weight_mean:.2f}',
                'ContentW': f'{content_weight_mean:.2f}',
                'Grad': f'{grad_norms[-1]:.3f}' if grad_norms else 'N/A'
            })

    pbar.close()

    # ========== Epochç»“æŸæ±‡æ€»ï¼ˆå¢å¼ºç‰ˆï¼‰==========
    avg_loss = total_loss / max(num_batches, 1)
    avg_acc = total_correct / max(total_samples, 1)

    # è®¡ç®—F1å’ŒAUC
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    avg_f1, avg_auc = 0.0, 0.5

    if len(all_labels) > 0:
        # F1-Score
        avg_f1 = f1_score((all_labels >= 0.5).astype(int),
                          (all_preds >= 0.5).astype(int),
                          average='weighted', zero_division=0)

        # ROC-AUC
        if len(np.unique(all_labels)) > 1:
            try:
                avg_auc = roc_auc_score((all_labels >= 0.5).astype(int), all_preds)
            except:
                avg_auc = 0.5

    # æƒé‡ç»Ÿè®¡
    if weight_stats:
        all_weights = torch.cat(weight_stats, dim=0)
        struct_mean = all_weights[:, 0].mean().item()
        struct_std = all_weights[:, 0].std().item()
        content_mean = all_weights[:, 1].mean().item()
        content_std = all_weights[:, 1].std().item()
    else:
        struct_mean = struct_std = content_mean = content_std = 0

    print(f"\n{'=' * 60}")
    print(f"ã€Epoch {epoch + 1} è®­ç»ƒæ€»ç»“ã€‘")
    print(f"ğŸ“Š Loss: {avg_loss:.4f} | Acc: {avg_acc:.2%} | F1: {avg_f1:.4f} | AUC: {avg_auc:.4f}")
    print(f"ğŸ¯ ç»“æ„æƒé‡: {struct_mean:.3f}Â±{struct_std:.3f}")
    print(f"ğŸ¯ å†…å®¹æƒé‡: {content_mean:.3f}Â±{content_std:.3f}")
    print(f"ğŸ§  æ¢¯åº¦èŒƒæ•°: {np.mean(grad_norms) if grad_norms else 0:.3f}")
    print(f"{'=' * 60}\n")

    return avg_loss, avg_acc, avg_f1, avg_auc


def validate_epoch(model, dataloader, device, config=None):
    """éªŒè¯æ¨¡å‹ï¼šå¢å¼ºç‰ˆ - è¿”å›å¤šæŒ‡æ ‡"""
    model.eval()
    total_loss = 0
    num_batches = 0
    all_preds = []
    all_labels = []

    if len(dataloader) == 0:
        print("âš ï¸ è­¦å‘Šï¼šéªŒè¯æ•°æ®åŠ è½½å™¨ä¸ºç©ºï¼")
        return 0.0, 0.0, 0.0, 0.0

    pbar = tqdm(dataloader, desc="éªŒè¯", file=sys.stdout, ncols=100)

    with torch.no_grad():
        for batch in pbar:
            struct_a = batch['struct_a'].to(device)
            struct_b = batch['struct_b'].to(device)
            content_a = batch['content_a'].to(device)
            content_b = batch['content_b'].to(device)
            similarity = batch['similarity'].to(device)

            # ä¿®æ”¹å‰å‘è°ƒç”¨ï¼Œæ”¶é›†é¢„æµ‹
            loss, similarities, _, _ = model.compute_loss(
                struct_a, content_a, struct_b, content_b, similarity
            )

            total_loss += loss.item()
            num_batches += 1

            # æ”¶é›†é¢„æµ‹å’Œæ ‡ç­¾
            all_preds.extend(similarities.cpu().numpy())
            all_labels.extend(similarity.cpu().numpy())

            avg_loss = total_loss / num_batches
            pbar.set_postfix({'Avg Loss': f'{avg_loss:.4f}'})

    pbar.close()

    # è®¡ç®—éªŒè¯æŒ‡æ ‡
    avg_loss = total_loss / max(num_batches, 1)
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)

    avg_acc = accuracy_score((all_labels >= 0.5).astype(int),
                             (all_preds >= 0.5).astype(int))
    avg_f1 = f1_score((all_labels >= 0.5).astype(int),
                      (all_preds >= 0.5).astype(int),
                      average='weighted', zero_division=0)
    avg_auc = 0.5
    if len(np.unique(all_labels)) > 1:
        try:
            avg_auc = roc_auc_score((all_labels >= 0.5).astype(int), all_preds)
        except:
            pass

    return avg_loss, avg_acc, avg_f1, avg_auc

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", default="config.yml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--epochs", type=int, help="è¦†ç›–é…ç½®ä¸­çš„è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, help="è¦†ç›–é…ç½®ä¸­çš„æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--lr", type=float, help="è¦†ç›–é…ç½®ä¸­çš„å­¦ä¹ ç‡")
    args = parser.parse_args()

    # åŠ è½½é…ç½®
    config = Config(args.config)

    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–
    if args.epochs:
        config._config['training']['epochs'] = args.epochs
    if args.batch_size:
        config._config['training']['batch_size'] = args.batch_size
    if args.lr:
        config._config['training']['learning_rate'] = args.lr

    # è®¾å¤‡
    device_cfg = config.get_dict('device')
    device = torch.device(
        f"cuda:{device_cfg.get('cuda_device', 0)}" if device_cfg.get(
            'auto_select') and torch.cuda.is_available() else "cpu"
    )
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆ›å»ºæ¨¡å‹
    model = EnhancedTableSimilarityModel(args.config).to(device)

    # ä¼˜åŒ–å™¨
    optimizer = optim.AdamW(
        model.parameters(),
        lr=config.get('training.learning_rate'),
        weight_decay=config.get('training.weight_decay')
    )

    # è°ƒåº¦å™¨
    scheduler_type = config.get('training.lr_scheduler.type', 'CosineAnnealingLR')
    if scheduler_type == 'CosineAnnealingLR':
        scheduler = optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=config.get('training.lr_scheduler.T_max')
        )
    elif scheduler_type == 'StepLR':
        scheduler = optim.lr_scheduler.StepLR(
            optimizer, step_size=5, gamma=config.get('training.lr_scheduler.gamma', 0.5)
        )

    # æ¸©åº¦è°ƒåº¦
    temp_scheduler = TemperatureScheduler(config) if config.get('training.temperature_scheduler.enabled') else None

    # è¯¾ç¨‹è°ƒåº¦
    curriculum_scheduler = CurriculumScheduler(config)

    # æ•°æ®åŠ è½½å™¨
    batch_size = config.get('training.batch_size')
    train_loader = get_dataloader(config_path=args.config, mode="train", batch_size=batch_size)
    val_loader = get_dataloader(config_path=args.config, mode="val", batch_size=batch_size)

    # æ—©åœ
    early_stopping = EarlyStoppingWithoutStop(config)

    # TensorBoard
    if config.get('training.logging.tensorboard_dir'):
        writer = SummaryWriter(config.get('training.logging.tensorboard_dir'))
    else:
        writer = None

    print("\n" + "=" * 60)
    print("å¼€å§‹è®­ç»ƒå¢å¼ºæ¨¡å‹...")
    print(f"æ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()):,}")
    print(f"è®­ç»ƒæ ·æœ¬: {len(train_loader.dataset)} | éªŒè¯æ ·æœ¬: {len(val_loader.dataset)}")
    print(f"è®­ç»ƒæ‰¹æ¬¡: {len(train_loader)} | æ‰¹æ¬¡å¤§å°: {batch_size}")
    print(f"è®­ç»ƒè½®æ•°: {config.get('training.epochs')}")
    print("=" * 60 + "\n")

    # è®­ç»ƒå†å²è®°å½•
    history = {
        'train_loss': [], 'val_loss': [],
        'train_acc': [], 'val_acc': [],
        'train_auc': [], 'val_auc': [],
        'train_f1': [], 'val_f1': []
    }

    for epoch in range(config.get('training.epochs')):
        # è¯¾ç¨‹å­¦ä¹ 
        threshold = curriculum_scheduler.get_threshold(epoch)
        if hasattr(train_loader.dataset, 'filter_similarity_threshold'):
            train_loader.dataset.filter_similarity_threshold = threshold

        # æ¸©åº¦è°ƒåº¦
        if temp_scheduler:
            current_temp = temp_scheduler.get_temperature(epoch)
            model.contrastive_enhancer.temperature = current_temp
        else:
            current_temp = config.get('model.contrastive_loss.temperature')

        print(f"\nEpoch {epoch + 1}/{config.get('training.epochs')} | é˜ˆå€¼: {threshold:.2f} | æ¸©åº¦: {current_temp:.4f}")
        print("-" * 60)

        # è®­ç»ƒä¸éªŒè¯ï¼ˆæ¥æ”¶å¤šè¿”å›å€¼ï¼‰
        train_loss, train_acc, train_f1, train_auc = train_epoch(
            model, train_loader, optimizer, device, epoch, temp_scheduler, config
        )
        val_loss, val_acc, val_f1, val_auc = validate_epoch(
            model, val_loader, device, config
        )

        # è®°å½•å†å²
        history['train_loss'].append(train_loss)
        history['val_loss'].append(val_loss)
        history['train_acc'].append(train_acc)
        history['val_acc'].append(val_acc)
        history['train_auc'].append(train_auc)
        history['val_auc'].append(val_auc)
        history['train_f1'].append(train_f1)
        history['val_f1'].append(val_f1)

        # è°ƒåº¦å™¨
        scheduler.step()

        # TensorBoardè®°å½•ï¼ˆå¢å¼ºï¼‰
        if writer:
            writer.add_scalar('Loss/Train', train_loss, epoch)
            writer.add_scalar('Loss/Val', val_loss, epoch)
            writer.add_scalar('Accuracy/Train', train_acc, epoch)
            writer.add_scalar('Accuracy/Val', val_acc, epoch)
            writer.add_scalar('F1/Train', train_f1, epoch)
            writer.add_scalar('F1/Val', val_f1, epoch)
            writer.add_scalar('AUC/Train', train_auc, epoch)
            writer.add_scalar('AUC/Val', val_auc, epoch)
            writer.add_scalar('LR', optimizer.param_groups[0]['lr'], epoch)
            writer.add_scalar('Temperature', current_temp, epoch)

        # æ—©åœ
        if config.get('training.early_stopping.enabled'):
            early_stopping.step(val_loss, model, epoch)

        print(f"\nEpoch {epoch + 1} æ€»ç»“:")
        print(f"  è®­ç»ƒ: Loss={train_loss:.4f}, Acc={train_acc:.2%}, F1={train_f1:.4f}, AUC={train_auc:.4f}")
        print(f"  éªŒè¯: Loss={val_loss:.4f}, Acc={val_acc:.2%}, F1={val_f1:.4f}, AUC={val_auc:.4f}")
        print(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")
        print(f"  æœ€ä½³è½®æ¬¡: {early_stopping.best_epoch} (æŸå¤±: {early_stopping.best_loss:.4f})")
        print("-" * 60)

    # è®­ç»ƒå®Œæˆåçš„å¯è§†åŒ–
    if writer:
        writer.close()

    # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
    def plot_training_curves(history, save_path='training_curves.png'):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿å›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 10))
        epochs = range(1, len(history['train_loss']) + 1)

        # Loss
        axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='è®­ç»ƒLoss', linewidth=2)
        axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='éªŒè¯Loss', linewidth=2)
        axes[0, 0].set_title('Losså˜åŒ–æ›²çº¿', fontsize=13, fontweight='bold')
        axes[0, 0].set_xlabel('Epoch');
        axes[0, 0].set_ylabel('Loss')
        axes[0, 0].legend();
        axes[0, 0].grid(alpha=0.3)

        # Accuracy
        axes[0, 1].plot(epochs, history['train_acc'], 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
        axes[0, 1].plot(epochs, history['val_acc'], 'r-', label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
        axes[0, 1].set_title('å‡†ç¡®ç‡å˜åŒ–æ›²çº¿', fontsize=13, fontweight='bold')
        axes[0, 1].set_xlabel('Epoch');
        axes[0, 1].set_ylabel('Accuracy')
        axes[0, 1].set_ylim(0, 1);
        axes[0, 1].legend();
        axes[0, 1].grid(alpha=0.3)

        # ROC-AUC
        axes[1, 0].plot(epochs, history['train_auc'], 'b-', label='è®­ç»ƒAUC', linewidth=2)
        axes[1, 0].plot(epochs, history['val_auc'], 'r-', label='éªŒè¯AUC', linewidth=2)
        axes[1, 0].set_title('ROC-AUCå˜åŒ–æ›²çº¿', fontsize=13, fontweight='bold')
        axes[1, 0].set_xlabel('Epoch');
        axes[1, 0].set_ylabel('AUC')
        axes[1, 0].set_ylim(0.5, 1);
        axes[1, 0].legend();
        axes[1, 0].grid(alpha=0.3)

        # F1-Score
        axes[1, 1].plot(epochs, history['train_f1'], 'b-', label='è®­ç»ƒF1', linewidth=2)
        axes[1, 1].plot(epochs, history['val_f1'], 'r-', label='éªŒè¯F1', linewidth=2)
        axes[1, 1].set_title('F1-Scoreå˜åŒ–æ›²çº¿', fontsize=13, fontweight='bold')
        axes[1, 1].set_xlabel('Epoch');
        axes[1, 1].set_ylabel('F1-Score')
        axes[1, 1].set_ylim(0, 1);
        axes[1, 1].legend();
        axes[1, 1].grid(alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\nâœ… è®­ç»ƒæ›²çº¿å·²ä¿å­˜è‡³ {save_path}")

    print("\n" + "=" * 60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š...")
    print("=" * 60)

    # æ‰§è¡Œç»˜å›¾
    plot_training_curves(history)

    best_info = early_stopping.get_best_model_info()
    print(f"\næœ€ä½³æ¨¡å‹åœ¨ç¬¬ {best_info['best_epoch']} è½®ï¼ŒéªŒè¯æŸå¤±: {best_info['best_loss']:.4f}")
    print("=" * 60)


if __name__ == "__main__":
    os.makedirs("models", exist_ok=True)
    main()
